{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd035d4a",
   "metadata": {},
   "source": [
    "# Tidy Data (continued)\n",
    "If you want to type along with me, use [this notebook](https://humboldt.cloudbank.2i2c.cloud/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fbethanyj0%2Fdata271_sp24&branch=main&urlpath=tree%2Fdata271_sp24%2Fdemos%2Fdata271_demo34_live.ipynb) instead. \n",
    "If you don't want to type and want to follow along just by executing the cells, stay in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e99e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fbaea6",
   "metadata": {},
   "source": [
    "## Revisit activity from last time\n",
    "**Activity 1**: Run the following cell to get a messy dataset. Tidy the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33cf95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Date': ['04/01/24', '04/02/24','04/03/24'],\n",
    "    'New York_Temperature': [32, 35, 33],\n",
    "    'New York_Humidity': [40, 45, 47],\n",
    "    'Los Angeles_Temperature': [70, 72, 71],\n",
    "    'Los Angeles_Humidity': [60, 65, 66]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cca8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt to put in long form\n",
    "df2 = df.melt(id_vars='Date',var_name='City_Var',value_name='Value')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ede664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables so that each column is single variable\n",
    "df2['City'] = df2.City_Var.str.split('_').str[0]\n",
    "df2['Variable'] = df2.City_Var.str.split('_').str[1]\n",
    "df2.drop(columns = 'City_Var',inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac111433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to get Temperature and Humidity as separate columns\n",
    "df2.pivot_table(index = ['Date','City'],columns = 'Variable',values = 'Value').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf778c5",
   "metadata": {},
   "source": [
    "**NOTE:** You can also use str.split with a `expand=True` argument to create multiple columns at once. Also, you can remove \"Variable\" as the label for the index. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.melt(id_vars='Date',var_name='City_Var',value_name='Value')\n",
    "df2[['City','Variable']] = df2.City_Var.str.split('_',expand=True) # create multiple columns at once\n",
    "df2.drop('City_Var',axis=1,inplace=True) # Drop the unneccessary column\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16322c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot\n",
    "tidy = df2.pivot_table(index = ['Date','City'],columns = 'Variable',values = 'Value')\n",
    "tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53816b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove the name for the columns\n",
    "tidy.columns.name= None \n",
    "tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8342577",
   "metadata": {},
   "source": [
    "### More Complex example: Tidying Billboard Top 100 Dataset\n",
    "The dataset shows the Billboard top hits around the year 2000. This dataset records the date a song first entered the Billboard Top 100. It has variables for artist, track, date entered, date peaked, genre, time, rank and week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c02e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"billboard.csv\", encoding=\"mac_latin2\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729aee2b",
   "metadata": {},
   "source": [
    "Upon first glance, the first seven columns look okay (we'll probably need to check the dtypes for those date columns though), but the next columns show weeks. Week is a variable. According to the principles of tidy data \"each variable is a column\", so Week should be a column. Lets use the `.melt` method to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5afe5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melting\n",
    "# We will use the first 7 columns as the identifier variables, use the name \"week\" for the variable column\n",
    "# and the name \"rank\" for the value column \n",
    "id_vars = [\"year\",\"artist.inverted\",\"track\",\"time\",\"genre\",\"date.entered\",\"date.peaked\"]\n",
    "df = pd.melt(frame=df,id_vars=id_vars, var_name=\"week\", value_name=\"rank\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dtypes to see what we're dealing with\n",
    "results = df.dtypes\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e3a7b",
   "metadata": {},
   "source": [
    "As expected, those dates aren't Pandas datetime type, so we'll want to address that eventually. Before that, the `week` column is looking messy. We should extract the week number from the string in the `week` column.  We can use regular expression to do that. \n",
    "\n",
    "Lets use the `extract` method ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html)) which allows us to extract regex capture groups from strings in a Pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aabfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the numbers\n",
    "df[\"week\"] = df['week'].str.extract('(\\d+)', expand=False).astype(int) # regex \\d+ matches one or more digits in \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c761804",
   "metadata": {},
   "source": [
    "Okay, now what are we working with? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ddf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846eefc1",
   "metadata": {},
   "source": [
    "There are quite a few null values in the rank column. This is because if a song is in the Top 100 for less than 76 weeks the remaining columns are filled with NaN. Let's remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd06b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning out unnecessary rows\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e0f3b",
   "metadata": {},
   "source": [
    "It is also strange for rank to be floats. They represent a position, so lets make them ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d378c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rank'] = df['rank'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd1a2a",
   "metadata": {},
   "source": [
    "As we closely examine the data now, we might notice another interesting thing. Let's look at the first two rows. Somehow they both have a value of 1 in the `week` column, but the first song enterred the billboard data in September of 2000 and the second one entered in February of 2000. Does week 1 correspond to the same date for both of those songs? No. The week columns shows the number of weeks *after* `date.entered`. So a `week` value of 1 does not correspond to the same date for all songs. If we want to put everything on the same scale of just `date` we will have to create that. Lets do this below. \n",
    "\n",
    "At this point, it will be useful to convert `date.entered` to a datetime type. We will also want to use the `to_timedelta` method ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html)) to combine the `week` information with the datetime information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085834ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"date\" columns\n",
    "df['date'] = pd.to_datetime(df['date.entered']) + pd.to_timedelta(df['week'], unit='w') - pd.DateOffset(weeks=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b7e67",
   "metadata": {},
   "source": [
    "Finally, a last nice step in tidying data is to keep only relevant columns and sort the data in a nice order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"artist.inverted\", \"track\", \"time\", \"genre\", \"week\", \"rank\", \"date\"]]\n",
    "df = df.sort_values(by=[\"date\",\"artist.inverted\",\"track\",\"week\",\"rank\"],ascending=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73694f0c",
   "metadata": {},
   "source": [
    "One last thing we notice about this is that some of the information about these songs is repeated many times. i.e. `artist.inverted`, `track`, `time`, and `genre` are all the same within a single song. It's really only the rank information that is changing. In this case we are dealing with one of the common problems of \"messy\" date: multiple types of observational units are stored in the same table.\n",
    "\n",
    "The two types of observational units here are song and rank. If we want to make this data a little tidier, we should split these into separate tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that contains the info for each unique song\n",
    "songs_cols = [\"artist.inverted\", \"track\", \"time\", \"genre\"]\n",
    "songs = df[songs_cols].drop_duplicates()\n",
    "songs = songs.reset_index(drop=True)\n",
    "songs[\"song_id\"] = songs.index\n",
    "songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe that contains the info about ranks through time\n",
    "ranks = df.merge(songs, on=[\"artist.inverted\", \"track\", \"time\", \"genre\"])\n",
    "ranks = ranks[[\"song_id\", \"date\",\"rank\"]]\n",
    "ranks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de65b0e",
   "metadata": {},
   "source": [
    "Now each dataframe contains a single type of observational value. There we can still get the information about specific song ranks (`song_id`is repeated and allows us to map to the song info), but now we aren't carrying around a whole bunch of repeated columns. This could make downstream analysis more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66976cbd",
   "metadata": {},
   "source": [
    "### Another example: Tuberculosis\n",
    "Hadley Wickham is a statistician that created the concept of \"tidy data\". In his paper, he used the following dataset about tubercolosis cases. The column names indicate whether the group is male or female and their age range.  For example m1524 means a male between the ages of 15 and 24, inclusive.\n",
    "\n",
    "Also, there is a distinction between zeros and missing values due to the data collection process, and this distinction is important. Lets tidy this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tb-raw.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22988282",
   "metadata": {},
   "source": [
    "Recall that one of the possible problems that occur in \"messy\" data is when multiple variables are stored in one column. We are dealing with this issue in this dataset since sex and age range are stored in columns. Let's tidy this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(df, id_vars=[\"country\",\"year\"], value_name=\"cases\", var_name=\"sex_and_age\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c1c49",
   "metadata": {},
   "source": [
    "We need to separate sex and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Sex, Age lower bound and Age upper bound group\n",
    "df[[\"sex\",\"age_lower\",'age_upper']] = df[\"sex_and_age\"].str.extract(\"(\\D)(\\d+)(\\d{2})\", expand=True)   # regular expression \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `age`column based on `age_lower` and `age_upper`\n",
    "df[\"age\"] = df[\"age_lower\"] + \"-\" + df[\"age_upper\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de458f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df.drop(['sex_and_age',\"age_lower\",\"age_upper\"], axis=1,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb7e49",
   "metadata": {},
   "source": [
    "At this point, our data follows the principles of tidy data. But we can follow up with some nice-to-dos as well by maybe dropping the null values and sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls and sort\n",
    "df = df.dropna()\n",
    "df = df.sort_values(by=[\"country\", \"year\", \"sex\", \"age\"],ascending=True)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
